---
layout: post
title: "선형대수 02 - 벡터와 행렬의 연산(2)"
tags: [데이터분석, 데이터사이언스, 선형대수]
comments: true
---
내적은 데이터 분석을 할 때 거의 필수적인 연산이므로, 잘 알아둬야 한다. 그럼 데이터 분석에서 벡터의 곱셈이 어떻게 활용되는지 살펴보자.<br />
<br />

### 04 데이터 분석에서 활용되는 벡터의 곱셈
- - -
##### 1) 가중합(weighted sum)
가중합이란 데이터 각각의 값에 어떤 가중치 값을 곱한 후 이 곱셈 결과들을 합한 것을 의미한다.<br />
$$ w_1 x_1 + \cdots + w_N x_N = \sum_{i=1}^N w_i x_i $$<br />
<br />

##### 2) 가중 평균(weighted average)
가중치값을 전체 가중치값의 합으로 나누어 가중합을 하는 것을 가중 평균이라고 한다.<br />

##### 3) 유사도(similarity)
유사도는 두 벡터가 닮은 정도를 수치적으로 나타내는 것으로, 많이 닮은 경우에는 내적의 값이 커지고 닮지 않은 경우에는 작아지는 특성을 가진다.<br />
<br />

##### 4) 선형 회귀 모형(linear regresiion model)
선형 회귀 모형이란, 독립 변수 $$x$$에서 종속 변수 $$y$$를 예측하기 위한 방법의 하나로 독립 변수 벡터 $$x$$와 가중치 벡터 $$w$$와의 가중합으로 $$y$$와 가장 비슷한 값 $$\hat{y}$$를 계산하는 수식을 말한다.<br />
$$\hat{y} = w_1 x_1 + \cdots + w_N x_N$$<br />
선형 회귀 모형은 **예측 모형** 중 가장 단순하면서도 널리 쓰이는 기본적인 모형이므로 꼭 알아둬야 한다. 예를 들어, 부동산 가격을 예측할 때 부동산 가격을 결정하는 여러 독립 변수들(평수, 지역, 역세권 등)이 있을 것이다. 하지만 현실 세계에서는 이러한 독립 변수들이 각각 얼마나 중요한지, 가중치 값들을 정확히 알 수 없다. 따라서 각각의 가중치들을 담은 $$w$$ 벡터 를 구하여, $$y$$와 가장 비슷한 값 $$\hat{y}$$을 예측하는 것이다.<br />
인공 신경망에서는 데이터를 노드(node) 혹은 뉴런(neuron)이라는 동그라미로 표시하고 곱셈은 선분 위의 숫자를 표기하여 나타낸다. 곱셈은 여러개의 선분이 만나는 것으로 표시한다.<br />
<br />

##### 5) 열 벡터의 선형 조합
$$ Xw=
\begin{bmatrix}
c_1 & c_2 & \cdots & c_M
\end{bmatrix}
\begin{bmatrix}
w_1 \\ w_2 \\ \vdots \\ w_M
\end{bmatrix}
=
w_1 c_1 + w_2 c_2 + \cdots + w_M c_M $$<br />
이러한 벡터의 선형 조합은 두 이미지 벡터를 섞는 **모핑(morphing)**에 사용되기도 한다.
<br />
<br />

##### 6) 여러 개의 벡터에 대한 가중합
$$
\begin{eqnarray}
\hat{y} = 
\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_M \\
\end{bmatrix}
&=& 
\begin{bmatrix}
w_1 x_{1,1} + w_2 x_{1,2} + \cdots + w_N x_{1,N} \\
w_1 x_{2,1} + w_2 x_{2,2} + \cdots + w_N x_{2,N} \\
\vdots  \\
w_1 x_{M,1} + w_2 x_{M,2} + \cdots + w_N x_{M,N} \\
\end{bmatrix}
\\
&=& 
\begin{bmatrix}
x_{1,1} & x_{1,2} & \cdots & x_{1,N} \\
x_{2,1} & x_{2,2} & \cdots & x_{2,N} \\
\vdots  & \vdots  & \vdots & \vdots \\
x_{M,1} & x_{M,2} & \cdots & x_{M,N} \\
\end{bmatrix}
\begin{bmatrix}
w_1 \\ w_2 \\ \vdots \\ w_N
\end{bmatrix}
\\
&=& 
\begin{bmatrix}
x_1^T \\
x_2^T \\
\vdots \\
x_M^T \\
\end{bmatrix}
\begin{bmatrix}
w_1 \\ w_2 \\ \vdots \\ w_N
\end{bmatrix}
\\
&=& X w 
\end{eqnarray}
$$<br />
결과적으로 5과 6은 모두 선형 회귀 모형에 사용되는, 같은 계산임을 확인할 수 있다.<br />
<br />

##### 7) 제곱합(sum of squares)
데이터의 분산이나 표준 편차를 구할 때, 각각의 데이터를 제곱한 후 이 값들을 모두 더한 **제곱합**을 사용해야 한다. 이는 $$x^Tx$$를 계산하여 구할 수 있다.<br />
<br />

##### 8) 잔차(Residual)
선형 회귀 모형을 통해 구한 예측치 $$\hat{y}$$은 현실의 실제 값(target)인 $$y$$와 차이가 있을 것이다. 이러한 차이를 오차(error) 혹은 잔차(residual)라고 한다. 흔히 $$e$$라고 표현하며, 모든 독립 변수 벡터에 대해 잔차값을 구하면 잔차 벡터 $$e$$를 찾을 수 있다.<br /> 
<br />

##### 9) 잔차 제곱합(RSS, residual sum of squares)
**잔차의 크기**는 잔차 벡터의 각 원소를 제곱한 후 더한 잔차 제곱합을 이용하여 구한다. 이 값은 $$eTe$$로 간단하게 쓸 수 있다.<br />
$$ \sum_{i=1}^{N} e_i^2 = \sum_{i=1}^{N} (y_i - w^Tx_i)^2 = e^Te =  (y - Xw)^T (y - Xw) = y^Ty - w^TX^Ty - y^TXw + w^TX^TXw $$<br />
<br />

##### 10) 이차 형식(Quadratic Form)
행 벡터 $$X^T$$와 정방행렬 $$A$$, 열 벡터 $$x$$를 순서대로 곱한 것을 이차 형식이라고 한다. 이를 계산하면, $$\sum_{i=1}^{N} \sum_{j=1}^{N} a_{i,j} x_i x_j$$인 **스칼라 값**이 된다.<br />
<br />

**참고 자료**<br />
[김도형 박사님의 데이터 사이언스 스쿨](www.datascienceschool.net)<br />

