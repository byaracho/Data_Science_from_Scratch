---
layout: post
title: "선형대수 03 - 연립방정식과 역행렬"
tags: [데이터분석, 데이터사이언스, 선형대수]
comments: true
---
### 01 행렬의 성질
- - -
##### 1) 행렬의 부호
행렬 전체의 부호는 정의하기 어렵지만, 부호와 비슷한 특성으로 **positive definite**라는 특성이 있다. 영 벡터가 아닌 모든 벡터 x에 대해 다음 부등식이 성립하면 행렬 A가 positive definite라고 한다.<br />
<br />
$$ x^T A x > 0 $$(등호를 포함하면 positive semi-definite)<br />
<br />

##### 2) 행렬의 크기
행렬의 부호와 마찬가지로, 크기와 비슷한 특성으로는 놈(norm), 대각합(trace), 행렬식(determinant)가 있다.<br />
<br />
##### 놈(norm)
행렬의 놈은 다음과 같이 정의 된다.<br />
<br />
$$ \Vert A \Vert_L = \left( \sum_{i=1}^M \sum_{j=1}^N |a_{ij}|^L \right)^{1/L} $$<br />
여기서 $$L$$은 1, 2 또는 무한대이며, 보통 $$L=2$$인 경우가 많이 쓰여서 L을 생략하기도 한다. L2놈은 **프로베니우스 놈(Frobenius norm)**이라고 불리기도 한다.<br />
<br />
또한 벡터에 대해서도 놈을 정의할 수 있는데, **벡터의 놈의 제곱은** 그 **벡터의 제곱합**과 같다.<br />
$$ \Vert x \Vert^2 = \sum_{i=1}^N x_{i}^2 = x^Tx $$<br />
선형대수학에서 놈 공간(normed space)은 벡터 공간의 각 원소의 놈을 통해 생기게 되는 벡터 공간을 의미한다.<br />
<br />

##### 대각합(trace)
대각합은 정방 행렬에 대해서반 정의대며 대각 원소들의 합을 의미한다. 대각합은 다음과 같은 성질이 있다.<br />
<br />
$$ \text{tr} (cA) = c\;\text{tr} (A) $$<br />
$$ \text{tr} (A^T) = \text{tr} (A) $$<br />
$$ \text{tr} (A + B) = \text{tr} (A) + \text{tr} (B) $$<br />
$$ \text{tr} (AB) = \text{tr} (BA) $$<br />
$$ \text{tr} (ABC) = \text{tr} (BCA) = \text{tr} (CAB) $$<br />
<br />
마지막 두 수식은 trace trick이라고하여 앞서 다룬 이차 형식(quadratic form)의 **미분**을 구하는데 유용하게 사용되니 기억해 두도록 하자.<br />
<br />
$$ x^TAx = \text{tr}(x^TAx) = \text{tr}(Axx^T)  = \text{tr}(xx^TA) $$<br />
<br />

##### 행렬식(determinant)
행렬식은 $$det(A)$$ 혹은 $$|A|$$라고 표현하며, **코팩터 전개(cofactor expanson)**이라고 불리는 **재귀적인 방법**으로 정의된다.<br />
<br />
$$ \det(A) = \sum_{i=1}^N \left\{ (-1)^{i+j_0}M_{i,j_0} \right\} a_{i,j_0} = \sum_{j=1}^N \left\{ (-1)^{i_0+j} M_{i_0,j} \right\} a_{i_0,j} $$<br />
<br />
여기서 $$ M_{i,j} $$는 마이너(minor, 소행렬식)이라고 하며, 정방 행렬 $$A$$에서 $$i$$행과 $$j$$열을 지워서 얻은 행렬의 행렬식이다. 마이너 또한 determinant이므로, 마찬가지로 위의 방법을 통해 2x2의 가장 작은 행렬의 행렬식을 구할 때까지 반복해야한다. 이러한 방법을 재귀적(recursive)라고 한다. 참고로 재귀(Recursion)은 수학이나 컴퓨터 과학 등에서 자기 자신을 정의할 때 자기 자신을 재참조하는 방법을 뜻한다.<br />
그리고 마이너에 $$(-1)^{i+j}$$를 곱한 값을 코팩터(cofactor, 여인수) C_{i,j}라고 한다.<br />
<br/>

이러한 방법을 통해 크기가 2, 3인 정방 행렬의 행렬식을 구하면 다음과 같다.<br />
<br />
$$ \det \left( \begin{bmatrix}a&b\\c&d\end{bmatrix} \right) =ad-bc$$<br />
$$ \det \left( \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix} \right) =aei+bfg+cdh-ceg-bdi-afh $$<br />
<br/>

행렬식은 다음과 같은 성질을 가진다.<br />
<br />
$$\det(A^{T}) = \det(A)$$<br />
$$\det(I) = 1$$<br />
$$\det(AB) = \det(A)\det(B)$$<br />
$$\det(A^{-1}) = \dfrac{1}{\det(A)}$$<br />
$$A^{-1} A = A A^{-1} = I$$<br />
$$\det(A A^{-1}) = \det(I) = 1$$<br />
$$\det(A) \det(A^{-1}) = 1$$<br />
<br />

##### 3) 개념들간의 관계
1. 행렬이 positive-definite이면 대각합이 양수이다.<br />
2. 행렬이 positive-definite이면 행렬식이 양수이다.<br />
3. 행렬이 positive-semidefinite이면 행렬식은 0이다.<br />
<br />

### 02 연립 방정식과 역행렬
- - -
앞서 살펴본 선형 예측 모형에서 연립 방정식과 (의사)역행렬을 이용하여 가중치 벡터를 구하는 방법을 알아보자.<br />
<br />
##### 1) 선형 연립 방정식(system of linear equations)
다음과 같이 M개의 미지수를 가지는 N개의 선형 방정식을 **선형 연립 방정식**이라고 한다.<br />
<br />
$$ \begin{matrix}
a_{11} x_1 & + \;& a_{12} x_2   &\; + \cdots + \;& a_{1M} x_M &\; = \;& b_1 \\
a_{21} x_1 & + \;& a_{22} x_2   &\; + \cdots + \;& a_{2M} x_M &\; = \;& b_2 \\
\vdots\;\;\; &   & \vdots\;\;\; &                & \vdots\;\;\; &     & \;\vdots \\
a_{N1} x_1 & + \;& a_{N2} x_2   &\; + \cdots + \;& a_{NM} x_M &\; = \;& b_N \\
\end{matrix} $$<br />
<br />
$$A_{N,M}$$를 행렬 A의 원소들이라고 하면, 이 식은 행렬의 곱셈을 이용하여 다음과 같이 간단하게 표현할 수 있다.<br />
<br />
$$ Ax = b $$<br />
<br />

##### 2) 역행렬(invertible matric)
$$ Ax = b $$의 $$x$$벡터를 찾기 위해서는 역행렬을 사용해야 한다. 정방 행렬 $$A$$의 역행렬 $$A^{-1}$$는 다음과 같은 관계를 만족하는 정방 행렬이다. 역행렬은 **행렬식이 0이 아닐 때**만 존재한다.<br />
<br />
$$A^{-1} A = A A^{-1} = I$$<br />
<br />
역행렬이 **존재**하는 행렬 : 가역행렬(invertible matrix), 정칙행렬regular matrix), 비특이행렬(non-singular matrix)<br />
역행렬이 **존재**하지 않는 행렬 : 비가역행렬(non-invertible), 특이행렬(singular matrix), 퇴화행렬(degenerate matrix)<br />
<br />

##### 3) 역행렬의 성질
$$(A^{T})^{-1} = (A^{-1})^{T}$$<br />
$$(AB)^{-1} = B^{-1} A^{-1}$$<br />
$$(ABC)^{-1} = C^{-1} B^{-1} A^{-1}$$<br />
<br />

##### 4) 역행렬의 계산
$$ A^{-1} = \dfrac{1}{\det A} C^T = \dfrac{1}{\det A} 
\begin{bmatrix}
C_{1,1} &\cdots& C_{N, 1}\\
\vdots &\ddots &\vdots\\
C_{1,N} &\cdots &C_{N,N}\\
\end{bmatrix} $$<br />
<br />
코팩터로 이루어진 행렬 $$C$$ : 코팩터 행렬(matrix of cofactors, 또는 cofactor matrix, comatrix)<br />
코팩터 행렬의 전치 행렬 $$C^{T}$$ : 어드조인트 행렬(adjoint matrix, adjugate matrix, 수반 행렬), $$adj(A)$$<br />
<br />

##### 5) 선형 연립 방정식의 해
$$x = A^{-1}b$$<br />
이는 선형 예측 모형에서 $$Xw = y$$의 가중치 벡터 $$w$$를 찾는데 사용할 수 있다.<br />
<br />

##### 최소 자승 문제(Least Square)
데이터 분석에서는 방정식의 수가 미지수의 수보다 많은 경우가 일반적이다. 이러한 경우에는 조건을 만족하는 해가 하나도 존재하지 않을 수 있다. 예를 들어 우리가 집값 예측 문제를 풀기 위해 독립 변수를 10개로 놓고, 최대한 많은 데이터들을 모으는 경우가 일반적이라는 것이다. 이러한 경우에는 b값을 가장 비슷하게 만들 수 있는 문제를 풀게 된다.<br />
<br />
$$Ax \approx b$$<br />
이러한 경우에는 좌변과 우변의 차이, **잔차 e**를 최소화 하는 문제로 바꾸어 풀 수 있다. 이를 최소화 한다는 것은 **잔차 벡터의 놈**을 최소화 하는 것과 같다.<br />
<br />
$$ e^Te = \Vert e \Vert^2 = (Ax-b)^T(Ax-b)$$ <br />
$$ x = \text{arg} \min_x e^Te = \text{arg} \min_x  \; (Ax-b)^T(Ax-b) $$<br />
<br />
이러한 문제를 **최소 자승 문제(Least Square)**라고 한다.<br />
<br />
최소 자승 문제의 답은 $$x = ((A^TA)^{-1}A^T) b$$이며, 행렬 $$((A^TA)^{-1}A^T)$$를 행렬 A의 **의사 역행렬(pseudo inverse, $$A^{+}$$)**라고 한다. 일단 지금은 이 답을 기억해두고, 나중에 행렬의 미분 공식을 사용하여 증명하도록 한다.<br />
<br />

**참고 자료**<br />
[김도형 박사님의 데이터 사이언스 스쿨](www.datascienceschool.net)<br />
[위키피디아-노름공간](https://ko.wikipedia.org/wiki/노름_공간)<br />
[위키피디아-재귀함수](https://ko.wikipedia.org/wiki/재귀함수)<br />


